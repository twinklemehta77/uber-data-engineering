# uber-data-engineering

First Step: Data Preparation

Initiated the project by working with a simulated Uber dataset. The initial step involved data cleaning processes such as removing duplicates, filling in missing values, and adjusting data types. Utilized Python programming for efficient and effective data cleaning.

Second Step: Cloud Infrastructure Setup

1. Established a compute engine on Google Cloud Platform (GCP) to handle the computational requirements of the project.
2. Created a storage bucket in GCP and uploaded the cleaned dataset to the bucket.
3. Accessed the compute engine through Secure Shell (SSH) and installed essential packages like Python, Pandas, and Mage to facilitate the subsequent steps in the data pipeline.

Third Step: Data Pipeline Development with Mage

1. Developed a data pipeline using Mage, focusing on Extract, Transform, and Load (ETL) processes to further refine and organize the cleaned data.
2. Established a connection between Google BigQuery and Mage using appropriate credentials, ensuring seamless data flow.

Fourth Step: Analytics and Insight Generation

1. Conducted analytical processes on the refined dataset, leveraging Mage capabilities.
2. Applied data analysis techniques to derive meaningful insights, ultimately creating a new table for detailed analysis.
3. Utilized Google Looker for comprehensive analysis and visualization, extracting valuable business insights from the processed data.

In summary, the project encompassed data cleaning, cloud infrastructure setup, data pipeline development, and insightful analysis, demonstrating a comprehensive and systematic approach to handling the Uber dataset.
